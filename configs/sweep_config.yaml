# defaults:
#   - override hydra/sweeper: optuna
# sampler:
#   _target_: optuna.samplers.TPESampler
# n_trials: 20
# n_jobs: 1

# model_checkpoint: "google/mt5-small" # str: e.g. "google/mt5-base" or "google/mt5-large" or "google/mt5-xl" or "google/mt5-xxl"
# model_name: "mt3-small-25k-baseline" # str
# device: "cuda" # str: "cuda" or "cpu"
# language: "da" # str: e.g. "da"
# wandb_mode: "run" # str: "run" or "dryrun"
# cache_dir: "/data/danish_summarization_danewsroom/cache_sara" # str
# redo_cache: "False" # bool: True or False. If True, will not use cached data

# text_column: "text" # str
# summary_column: "summary" # str
# train_path: "/data/danish_summarization_danewsroom/clean25k.csv" # "/data/danish_summarization_danewsroom/train25k_clean.csv" # str
# val_path: "/data/danish_summarization_danewsroom/clean25k.csv" # "/data/danish_summarization_danewsroom/val25k_clean.csv" # str
# test_path: "/data/danish_summarization_danewsroom/clean25k.csv" # "/data/danish_summarization_danewsroom/test25k_clean.csv" # str
# max_input_length: "1024" # int:  max text (article) max token length
# max_target_length: "128" # int: max reference summary max token length

program: train.py
method: bayes
metric:
  goal: minimize
  name: loss
parameters:
#   min_length: "15" # int: min summary length
#   max_length: "128" # int: max summary length
#   num_beams:
#     distribution: int_uniform
#     max: 6
#     min: 4
#   no_repeat_ngram_size:
#     distribution: int_uniform
#     max: 5
#     min: 2
#   length_penalty:
#     distribution: int_uniform
#     max: 10
#     min: 4
#   early_stopping: "True" # bool: early stopping
#   dropout_rate:
#     distribution: uniform
#     max: 0.3
#     min: 0.15

# #  training:
#   output_dir: "/data/danish_summarization_danewsroom/models" # str: output directory
#   overwrite_output_dir: "True" # bool: overwrite output directory
#   evaluation_strategy: "steps" # str: "steps" or "epoch"
#   save_strategy: "steps" # str: "steps" or "epoch"
  training.learning_rate: # float: learning rate
    distribution: uniform
    max: 0.0005
    min: 5e-06
#   lr_scheduler_type:
#     distribution: categorical
#     values:
#       - "linear"
#       - "constant"
#       - "cosine"
#       - "polynomial"
#   #per_device_train_batch_size: 8 # int: batch size
#   #per_device_eval_batch_size: 8 # int: batch size
#   logging_steps: "100" # int: log every x steps
#   save_steps: "200" # int: save every x steps
#   eval_steps: "200" # int: evaluate every x steps
#   warmup_steps: "100" # int: warm up steps
#   save_total_limit: "1" # int: number of checkpoints to save
#   num_train_epochs:
#     distribution: int_uniform
#     max: 10
#     min: 1
#   predict_with_generate: "True" # bool: predict with generate
#   fp16: "True" # bool: use fp16
#   load_best_model_at_end: "True" # bool: load best model at end
#   metric_for_best_model: "loss" # str: metric for best model
#   max_grad_norm:
#     distribution: int_uniform
#     max: 10
#     min: 5
#   pad_to_multiple_of: "8" # int: pad to multiple of


